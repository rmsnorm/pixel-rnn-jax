{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diagonal_bilstm import DiagonalBiLSTM\n",
    "from flax import nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import treescope\n",
    "import tqdm\n",
    "import os\n",
    "treescope.register_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# downloaded from \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "dataset_path = '/home/apoorv/Projects/datasets/cifar-10-batches-py/'\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "cifar10_batches = []\n",
    "for i in range(1, 6):\n",
    "    file_path = os.path.join(dataset_path, f'data_batch_{i}')\n",
    "    batch = unpickle(file_path)\n",
    "    cifar10_batches.append(batch)\n",
    "\n",
    "cifar10_test_batches = []\n",
    "file_path = os.path.join(dataset_path, 'test_batch')\n",
    "test_batch = unpickle(file_path)\n",
    "cifar10_test_batches.append(test_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(cifar10_batches):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for batch in cifar10_batches:\n",
    "        images = batch[b'data']\n",
    "        images = images.reshape(images.shape[0], 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "        data.append(images)\n",
    "        labels.append(batch[b'labels'])\n",
    "    data = np.row_stack(data)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = get_images(cifar10_batches)\n",
    "test_data, test_labels = get_images(cifar10_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image_grid(images, grid_shape):\n",
    "    assert images.shape[0] == grid_shape[0] * grid_shape[1]\n",
    "    fig, axes = plt.subplots(grid_shape[0], grid_shape[1], figsize=grid_shape)\n",
    "    fig.patch.set_facecolor('black') # Set background to black for better contrast\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img_to_show = images[i]\n",
    "        ax.imshow(img_to_show)\n",
    "        ax.axis('off') # Hide the x and y axes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_grid(data[:16], (4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self, images, batch_size):\n",
    "        self.images = images\n",
    "        self.bsz = batch_size\n",
    "        self.iter = 0\n",
    "    \n",
    "    def get_batch(self):\n",
    "        if (self.iter + 1) * self.bsz < self.images.shape[0]:\n",
    "            start_idx = self.iter * self.bsz\n",
    "            end_idx = (self.iter + 1) * self.bsz\n",
    "            self.iter += 1\n",
    "            return self.images[start_idx: end_idx].astype(jnp.uint8)\n",
    "        else:\n",
    "            self.shuffle()\n",
    "            return self.get_batch()\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.images = np.random.shuffle(self.images)\n",
    "        self.iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, image_batch):\n",
    "    labels = nnx.one_hot(image_batch, 256)\n",
    "    logits_bmncd = model(image_batch)\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(3):\n",
    "        loss += optax.softmax_cross_entropy(logits_bmncd[:, :, :, i], labels[:, :, :, i])\n",
    "\n",
    "    loss = jnp.mean(jnp.mean(loss, axis=[1, 2]))\n",
    "    return loss\n",
    "\n",
    "def compute_accuracy(logits_bmncd, image_batch):\n",
    "    red_accuracy = jnp.argmax(logits_bmncd[:, :, :, 0], axis=-1) == image_batch[:, :, :, 0]\n",
    "    green_accuracy = jnp.argmax(logits_bmncd[:, :, :, 1], axis=-1) == image_batch[:, :, :, 1]\n",
    "    blue_accuracy = jnp.argmax(logits_bmncd[:, :, :, 2], axis=-1) == image_batch[:, :, :, 2]\n",
    "\n",
    "    top1_accuracy = red_accuracy & green_accuracy & blue_accuracy\n",
    "    top1_accuracy = jnp.mean(jnp.mean(top1_accuracy, axis=[1, 2]))\n",
    "\n",
    "def compute_nll(logits_bmncd, im_bmnc):\n",
    "    # logits_bmnh , where h = 3*256\n",
    "    # compute negative log-likelihood of a batch\n",
    "    logprob_bmn = 0\n",
    "    for i in range(3):\n",
    "        logprob_bmnd = nnx.log_softmax(logits_bmncd[:,:,:,i])\n",
    "        logprob_bmn += logprob_bmnd[im_bmnc[:,:,:,i]]\n",
    "    return -jnp.mean(jnp.sum(logprob_bmn, axis=[1, 2]))\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, batch):\n",
    "    grad_fn = nnx.value_and_grad(compute_loss)\n",
    "    loss, grads = grad_fn(model, batch)\n",
    "    optimizer.update(grads)\n",
    "    return loss\n",
    "\n",
    "def compute_metrics(model, image_batch):\n",
    "    labels = nnx.one_hot(image_batch, 256)\n",
    "    logits_bmncd = model(image_batch)\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(3):\n",
    "        loss += optax.softmax_cross_entropy(logits_bmncd[:, :, :, i], labels[:, :, :, i])\n",
    "    loss = jnp.mean(jnp.mean(loss, axis=[1, 2]))\n",
    "\n",
    "    top1_accuracy = compute_accuracy(logits_bmncd, image_batch)\n",
    "    \n",
    "    nll = compute_nll(logits_bmncd, image_batch)\n",
    "\n",
    "    return loss, top1_accuracy, nll\n",
    "\n",
    "@nnx.jit()\n",
    "def eval_step(model, batch, eval_metrics):\n",
    "    loss, top1_accuracy, nll = compute_metrics(model, batch)\n",
    "\n",
    "    eval_metrics.update(\n",
    "        loss=loss,\n",
    "        accuracy=top1_accuracy,\n",
    "        nll=nll\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcher = Batcher(data, 1)\n",
    "batch = batcher.get_batch()\n",
    "\n",
    "plt.imshow(batch[0])\n",
    "\n",
    "# model hypers\n",
    "features = 64\n",
    "num_layers = 1\n",
    "is_rgb = True\n",
    "preds_dim = 256\n",
    "enable_skip_connections = True\n",
    "\n",
    "rngs = nnx.Rngs(params=0)\n",
    "model = DiagonalBiLSTM(features=features,\n",
    "                       num_layers=num_layers,\n",
    "                       is_rgb=is_rgb,\n",
    "                       preds_dim=preds_dim,\n",
    "                       enable_skip_connections=enable_skip_connections,\n",
    "                       rngs=rngs)\n",
    "\n",
    "grad_clip = 1.0\n",
    "optimizer = nnx.Optimizer(\n",
    "    model,\n",
    "    optax.chain(\n",
    "        optax.clip_by_global_norm(grad_clip),\n",
    "        optax.adam(learning_rate=1e-3)\n",
    "    )\n",
    ")\n",
    "\n",
    "for i in range(1000):\n",
    "    loss = train_step(model, optimizer, batch)\n",
    "    print(f\"loss[{i}]: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_image_batch = model.generate(32, 32, 1, jax.random.key(123))\n",
    "plt.imshow(gen_image_batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on entire CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_format = \"{desc}[{n_fmt}/{total_fmt}]{postfix} [{elapsed}<{remaining}]\"\n",
    "batch_size = 64\n",
    "train_total_steps = data.shape[0] // batch_size\n",
    "num_epochs = 100\n",
    "eval_total_steps = test_data.shape[0] // batch_size\n",
    "\n",
    "\n",
    "eval_metrics = nnx.MultiMetric(\n",
    "    loss=nnx.metrics.Average('loss'),\n",
    "    accuracy=nnx.metrics.Average('accuracy'),\n",
    "    nll=nnx.metrics.Average('nll')\n",
    ")\n",
    "\n",
    "train_metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "}\n",
    "\n",
    "eval_metrics_history = {\n",
    "    \"test_loss\": [],\n",
    "    \"test_accuracy\": [],\n",
    "    \"test_nll\": []\n",
    "}\n",
    "\n",
    "def train_one_epoch(epoch, model, optimizer):\n",
    "    model.train()  # Set model to the training mode: e.g. update batch statistics\n",
    "    with tqdm.tqdm(\n",
    "        desc=f\"[train] epoch: {epoch}/{num_epochs}, \",\n",
    "        total=train_total_steps,\n",
    "        bar_format=bar_format,\n",
    "        leave=True,\n",
    "    ) as pbar:\n",
    "        batcher = Batcher(data, batch_size)\n",
    "        batcher.shuffle()\n",
    "        for i in range(train_total_steps):\n",
    "            batch = batcher.get_batch()\n",
    "            loss = train_step(model, optimizer, batch)\n",
    "            train_metrics_history[\"train_loss\"].append(loss.item())\n",
    "            wandb.log({\"train_loss\": loss.item(), \"samples\": epoch * train_total_steps * batch_size + (i+1) * batch_size})\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "def evaluate_model(epoch, model):\n",
    "    # Compute the metrics on the train and val sets after each training epoch.\n",
    "    model.eval()  # Set model to evaluation model: e.g. use stored batch statistics\n",
    "    with tqdm.tqdm(\n",
    "        desc=f\"[eval] epoch: {epoch}/{num_epochs}, \",\n",
    "        total=eval_total_steps,\n",
    "        bar_format=bar_format,\n",
    "        leave=True,\n",
    "    ) as pbar:\n",
    "        eval_metrics.reset()  # Reset the eval metrics\n",
    "        batcher = Batcher(test_data, batch_size)\n",
    "        for i in range(eval_total_steps):\n",
    "            batch = batcher.get_batch()\n",
    "            eval_step(model, batch, eval_metrics)\n",
    "            pbar.update(1)\n",
    "\n",
    "        for metric, value in eval_metrics.compute().items():\n",
    "            wandb.log({f'test_{metric}': value})\n",
    "            eval_metrics_history[f'test_{metric}'].append(value)\n",
    "\n",
    "    print(f\"[eval] epoch: {epoch}/{num_epochs}\")\n",
    "    print(f\"- total loss: {eval_metrics_history['test_loss'][-1]:0.4f}\")\n",
    "    print(f\"- Accuracy: {eval_metrics_history['test_accuracy'][-1]:0.4f}\")\n",
    "    print(f\"- NLL: {eval_metrics_history['test_nll'][-1]:0.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
